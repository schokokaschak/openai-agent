from deepeval import evaluate
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.metrics import GEval
from deepeval.dataset import EvaluationDataset, Golden
from app.agent import run_agent

# Define a GEval metric instance
# GEval evaluates correctness of LLM outputs based on given expected outputs
# - 'criteria' explains how correctness is judged
# - 'evaluation_params' specifies which fields of the LLMTestCase are evaluated
# - 'threshold' defines the minimum score to be considered correct
metric = GEval(
    name="Correctness",
    criteria=(
        "Determine if the 'actual output' is correct based on the 'expected output'. "
        "It has to be correct in essence. Ignore format issues."
    ),
    evaluation_params=[
        LLMTestCaseParams.ACTUAL_OUTPUT,  # Evaluate the actual output of the agent
        LLMTestCaseParams.EXPECTED_OUTPUT,  # Compare against the expected output
    ],
    threshold=0.5,
)

# Define some golden test cases (questions with their correct answers)
goldens = [
    Golden(input="What is the capital of France?", expected_output="Paris"),
    Golden(input="What is 14 * 1?", expected_output="14"),
]

# Create an evaluation dataset from the golden examples
dataset = EvaluationDataset(goldens=goldens)

# For each golden example, generate a test case using the agent's actual output
# - 'actual_output' is generated by calling your agent with the golden input
for golden in dataset.goldens:
    dataset.add_test_case(
        LLMTestCase(
            input=golden.input,  # The question to ask the agent
            expected_output=golden.expected_output,  # The correct answer
            actual_output=run_agent(golden.input)["summary"],  # Agent's response
        )
    )

# Run the evaluation on all test cases using the defined metric
evaluate(test_cases=dataset.test_cases, metrics=[metric])
